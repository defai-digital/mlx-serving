# Vision Model Fair Benchmark Configuration
# Compares mlx-vlm (baseline) vs mlx-serving (our implementation)
# Run: npx tsx benchmarks/compare-vision-fair.ts

benchmark:
  max_tokens: 100
  temperature: 0.7
  timeout_ms: 300000  # 5 minutes

# Vision models to test (small to large)
# All Qwen VL models are known to work well with MLX
models:
  - name: "mlx-community/Qwen2-VL-2B-Instruct-4bit"
    size: "2B"
    questions: 5
    cycles: 1
    enabled: true

  - name: "mlx-community/Qwen3-VL-4B-Instruct-4bit"
    size: "4B"
    questions: 5
    cycles: 1
    enabled: true

  - name: "mlx-community/Qwen2.5-VL-7B-Instruct-4bit"
    size: "7B"
    questions: 5
    cycles: 1
    enabled: true

  - name: "mlx-community/Qwen3-VL-8B-Instruct-4bit"
    size: "8B"
    questions: 5
    cycles: 1
    enabled: true

  - name: "mlx-community/Qwen2-VL-72B-Instruct-4bit"
    size: "72B"
    questions: 5
    cycles: 1
    enabled: false  # Disabled by default (very large model, 40GB+)

# Test images (all from benchmarks/test-images/)
test_images:
  - "benchmarks/test-images/colors.jpg"
  - "benchmarks/test-images/shapes.jpg"
  - "benchmarks/test-images/numbers.jpg"
  - "benchmarks/test-images/text1.jpg"
  - "benchmarks/test-images/math.jpg"

# Test prompts - vision-specific questions
prompts:
  - "Describe this image in detail."
  - "What objects do you see in this image?"
  - "What colors are present in this image?"
  - "Read any text visible in this image."
  - "What is the main subject of this image?"
