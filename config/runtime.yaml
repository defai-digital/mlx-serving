# Kr-MLX-LM Runtime Configuration
# All configurable parameters in one place

# Request Batching Configuration (Week 1 Feature + Week 2 Day 3 Enhancements + High-Concurrency Optimization 2025-10-27)
# BUGFIX: Disabled to fix intermittent Python crash after 5 requests
batch_queue:
  enabled: false                 # DISABLED: Testing if OpsMultiplexer causes race condition
  max_batch_size: 20            # Maximum requests per batch (optimized for high concurrency: 10 → 20)
  flush_interval_ms: 2          # Maximum wait time before flushing (optimized: 5ms → 2ms)

  # Week 2 Day 3: Advanced Features
  adaptive_sizing: true         # Enable adaptive batch size adjustment
  target_batch_time_ms: 10      # Target batch processing time for adaptive sizing
  priority_queue: true          # Enable priority queue (high/normal/low) - ENABLED for API server

# Generate Request Batching (v1.3.0 - IPC Reduction Feature)
# Batches multiple generate() requests into a single batch_generate() RPC call
# Target: ≥90% IPC reduction under concurrent load
# BUGFIX: Disabled to fix intermittent race condition causing stream cancellations
generate_batcher:
  enabled: false                       # DISABLED: Race condition in batching logic
  min_batch_size: 2                    # Minimum requests before dispatch
  max_batch_size: 16                   # Maximum requests per batch
  min_hold_ms: 0.75                    # Minimum hold window (ms)
  max_hold_ms: 3                       # Maximum hold window (ms)
  background_hold_extension_ms: 2      # Extra hold time for background priority requests
  target_batch_time_ms: 12             # Target batch processing time for adaptive sizing
  pause_on_backpressure_ms: 20         # Pause duration when StreamRegistry signals backpressure

# Phase 1: Request Deduplication (Performance Optimization v0.1.0-alpha.1)
# Collapses identical concurrent requests into shared Promise
# Target: +10-30% throughput for duplicate-heavy workloads
request_deduplication:
  enabled: false                    # DISABLED: Enable after testing
  ttl_ms: 1000                      # Time-to-live for dedupe cache (1 second)
  max_entries: 1000                 # Maximum dedupe cache entries
  max_payload_bytes: 10240          # Maximum payload size (10KB)

# Phase 1: Prompt Cache (Performance Optimization v0.1.0-alpha.1)
# LRU cache for prompt responses with size-aware eviction
# Target: >80% cache hit rate for repeated prompts
prompt_cache:
  enabled: false                    # DISABLED: Enable after testing
  max_entries: 10000                # Maximum cache entries (LRU eviction)
  max_total_tokens: 1000000         # Maximum total tokens cached
  max_total_bytes: 104857600        # Maximum total bytes (100MB)
  ttl_ms: 300000                    # Time-to-live for cache entries (5 minutes)
  cleanup_interval_ms: 60000        # Cleanup interval (1 minute)

  # Optional disk persistence
  persistence:
    enabled: false                  # DISABLED: Enable for persistent cache
    path: 'automatosx/tmp/prompt-cache.json'
    save_interval_ms: 300000        # Save every 5 minutes

# Phase 1: Request Coalescing (Performance Optimization v0.1.0-alpha.1)
# Merges identical concurrent requests (N→1 inference, N responses)
# Target: >50% request reduction for concurrent duplicates
request_coalescing:
  enabled: false                    # DISABLED: Enable after testing
  max_subscribers: 100              # Maximum subscribers per coalesced request
  timeout_ms: 5000                  # Coalescing window timeout (5 seconds)

# Continuous Batching (Week 2 Feature)
# Dynamic batch composition - requests join/leave independently
# No head-of-line blocking, continuous GPU utilization
# Target: 3-5x throughput improvement over static batching
continuous_batching:
  # Maximum requests in a single batch
  max_batch_size: 8

  # How long to wait for more requests before starting generation (ms)
  # Lower = better latency, Higher = better throughput
  batch_window_ms: 10.0

  # Dynamically adjust batch size based on GPU utilization
  adaptive_sizing: true

  # Target GPU utilization (%) for adaptive sizing
  target_gpu_utilization: 85

  # Minimum batch size (even if no requests waiting)
  # Set to 1 to never wait for full batches
  min_batch_size: 1

# Python Runtime Process Management
python_runtime:
  # Path to Python executable (relative to project root)
  python_path: '.kr-mlx-venv/bin/python'

  # Path to runtime script (relative to project root)
  runtime_path: 'python/runtime.py'

  # Maximum restart attempts on crash
  max_restarts: 3

  # Process startup timeout (milliseconds)
  startup_timeout_ms: 30000  # 30 seconds

  # Graceful shutdown timeout (milliseconds)
  shutdown_timeout_ms: 5000  # 5 seconds

  # Fallback probe timeout for Python initialization (milliseconds)
  # Primary trigger is event-driven (Python "ready" signal)
  # This timeout only fires if Python fails to emit ready signal
  # OPTIMIZATION: Increased from 500ms to 3000ms as this is now a safety fallback only
  init_probe_fallback_ms: 3000  # 3 seconds (fallback only)

  # Base delay for restart exponential backoff (milliseconds)
  restart_delay_base_ms: 1000  # 1 second

# JSON-RPC Transport Configuration
json_rpc:
  # Default request timeout (milliseconds)
  default_timeout_ms: 300000  # 300 seconds (5 minutes) for large models like Gemma 27B

  # Maximum line buffer size for incomplete JSON (bytes)
  max_line_buffer_size: 65536  # 64KB

  # Maximum pending requests before backpressure
  max_pending_requests: 100

  # Retry policy for idempotent JSON-RPC calls
  retry:
    max_attempts: 3
    initial_delay_ms: 100
    max_delay_ms: 5000
    backoff_multiplier: 2.0
    jitter: 0.25
    retryable_errors:
      - TIMEOUT
      - ECONNRESET

  # Circuit breaker to guard Python runtime availability
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout_ms: 10000
    half_open_max_calls: 1
    half_open_success_threshold: 2
    failure_window_ms: 60000

# Stream Registry Configuration
stream_registry:
  # Default stream timeout (milliseconds)
  default_timeout_ms: 300000  # 5 minutes

  # Maximum concurrent active streams
  max_active_streams: 10

  # Cleanup interval for expired streams (milliseconds)
  cleanup_interval_ms: 60000  # 1 minute

  # Phase 4: Stream Optimization (v0.2.0)
  # Adaptive stream limits
  # BUGFIX: Disabled adaptive limits to prevent premature stream rejection
  # The min_streams: 5 was causing stream registration failures in sequential workloads
  # where completed streams may not be cleaned up before the next request arrives
  adaptive_limits:
    enabled: false  # Disabled until stream cleanup timing is fixed
    min_streams: 1  # Changed from 5 to 1 to allow single-stream workloads
    max_streams: 50
    target_ttft_ms: 1000  # Target time to first token
    target_latency_ms: 100  # Target queue latency
    adjustment_interval_ms: 5000  # How often to adjust limits
    scale_up_threshold: 0.8  # Scale up when utilization > 80%
    scale_down_threshold: 0.3  # Scale down when utilization < 30%

  # Chunk pooling for memory efficiency
  chunk_pooling:
    enabled: true
    pool_size: 1000  # Max pooled chunk objects
    pool_cleanup_interval_ms: 30000  # Clean pool every 30s

  # Backpressure control (ACK/credit flow)
  backpressure:
    enabled: true
    max_unacked_chunks: 100  # Max chunks before blocking
    ack_timeout_ms: 5000  # Timeout waiting for ACK
    slow_consumer_threshold_ms: 1000  # Warn if consumer is slow

  # Per-stream metrics tracking
  metrics:
    enabled: true
    track_ttft: true  # Time to first token
    track_throughput: true  # Tokens per second
    track_cancellations: true  # Cancellation rates
    export_interval_ms: 10000  # Export metrics every 10s

streaming:
  phase4:
    adaptive_governor:
      enabled: false
      target_ttft_ms: 550
      max_concurrent: 80
      min_concurrent: 16
      pid:
        kp: 0.35
        ki: 0.08
        kd: 0.15
        integral_saturation: 200
        sample_interval_ms: 200
      cleanup:
        sweep_interval_ms: 100
        max_stale_lifetime_ms: 500
      tenant_budgets:
        default:
          tenant_id: default
          hard_limit: 20
          burst_limit: 32
          decay_ms: 60000

# Model Configuration
model:
  # Default context length when not specified
  default_context_length: 8192

  # Default max tokens for generation
  default_max_tokens: 512

  # Maximum number of loaded models (memory management)
  max_loaded_models: 5

  # Supported model dtypes
  supported_dtypes:
    - 'float16'
    - 'bfloat16'
    - 'float32'

  # Default quantization mode
  default_quantization: 'none'  # Options: none, int8, int4

  # Default dtype
  default_dtype: 'unknown'

  # Security: Trusted model directories (optional)
  # If specified, local_path must be within one of these directories
  # If empty/null, all absolute paths are allowed (less secure but more flexible)
  # Example: ['/Users/username/models', '/opt/mlx-models']
  trusted_model_directories: null

  # Security: Maximum generation tokens (prevents DoS)
  max_generation_tokens: 4096

  # Security: Maximum temperature value
  max_temperature: 2.0

  # Phase 2: In-Memory Model Caching (v0.2.0)
  # Keep loaded models in RAM for instant reuse
  # Provides 50-70% load time reduction for warm loads
  memory_cache:
    # Enable in-memory model caching
    enabled: true

    # Maximum models to keep loaded in memory
    # Uses LRU (Least Recently Used) eviction when limit reached
    # Each model ~3-4GB RAM (4-bit quantized), adjust based on available memory
    max_cached_models: 5

    # Eviction strategy: 'lru' (least-recently-used)
    eviction_strategy: 'lru'

    # Models to warmup on engine startup (optional)
    # These models will be preloaded into memory during engine.start()
    # Format: ['model-id', 'org/model-name', ...]
    warmup_on_start: []

    # Track cache statistics for observability
    track_stats: true

# Model Artifact Cache Configuration (v0.2.0 Phase 2)
# Persistent disk cache for model weights, tokenizers, and configs
# Provides 90%+ load time reduction by caching HuggingFace downloads
cache:
  # Enable persistent artifact cache
  enabled: true

  # Directory to store cached artifacts (relative to project root)
  cache_dir: '.kr-mlx-cache'

  # Maximum cache size in bytes (default: 100GB)
  # Models are evicted using LRU policy when this limit is exceeded
  max_size_bytes: 107374182400  # 100GB

  # Maximum age of cache entries in days
  # Entries older than this are eligible for eviction
  max_age_days: 30

  # Eviction policy when cache is full
  # Options: 'lru' (least-recently-used), 'lfu' (least-frequently-used), 'fifo' (first-in-first-out)
  eviction_policy: 'lru'

  # Models to preload on startup (optional)
  # These models will be loaded into cache during engine initialization
  # Format: ['model-id', 'org/model-name', ...]
  preload_models: []

  # Enable cache validation on startup
  # Checks for corrupted entries and removes them from index
  validate_on_startup: true

  # Enable cache compression (future feature - not yet implemented)
  enable_compression: false

# Python Bridge IPC Configuration
python_bridge:
  # stdin buffer overflow limit (bytes)
  max_buffer_size: 1048576  # 1MB

  # asyncio.Queue maxsize for token streaming
  stream_queue_size: 100

  # Maximum retries for queue.put backpressure
  queue_put_max_retries: 100

  # Backoff delay for queue.put retry (milliseconds)
  queue_put_backoff_ms: 10  # 10ms

# Outlines Adapter Configuration
outlines:
  # Maximum schema size (bytes) - prevents compile overhead
  max_schema_size_bytes: 32768  # 32KB

# Performance Tuning
performance:
  # Enable aggressive garbage collection after model operations
  aggressive_gc: false

  # Enable IPC message batching (future feature)
  enable_batching: false

  # Batch size for IPC messages
  batch_size: 10

  # Batch timeout (milliseconds)
  batch_timeout_ms: 50

  # Use MessagePack instead of JSON for IPC (future feature)
  use_messagepack: false

# Telemetry & Monitoring (Phase 4)
telemetry:
  # Enable OpenTelemetry metrics collection
  enabled: false  # Set to true to enable metrics

  # Service name for metrics (appears in Prometheus)
  service_name: 'kr-serve-mlx'

  # Prometheus exporter port
  prometheus_port: 9464

  # Metrics export interval (milliseconds)
  export_interval_ms: 60000  # 60 seconds

# Development & Debugging
development:
  # Enable verbose logging
  verbose: false

  # Enable debug mode
  debug: false

  # Log all IPC messages
  log_ipc: false

  # Enable performance profiling
  enable_profiling: false

# Phase 2: Multi-Worker Routing (Performance Optimization v0.2.0)
# Enables multiple Python worker processes with intelligent routing
runtime_router:
  enabled: false                      # DISABLED: Enable after testing
  worker_count: 3                     # Number of Python workers
  routing_strategy: 'round-robin'     # Options: 'round-robin', 'least-busy'
  health_check_interval_ms: 5000      # Health check interval (5 seconds)
  worker_restart_delay_ms: 1000       # Base delay for worker restart
  sticky_session_enabled: true        # Enable sticky sessions for streaming
  sticky_session_ttl_ms: 300000       # Sticky session TTL (5 minutes)

# Python Runtime Manager (Phase 2: Multi-Worker)
python_runtime_manager:
  enabled: false                      # DISABLED: Enable after testing
  worker_count: 3                     # Number of Python workers
  heartbeat_interval_ms: 5000         # Worker heartbeat interval (5 seconds)
  heartbeat_timeout_ms: 10000         # Heartbeat timeout (10 seconds)

# Adaptive Batch Coordinator (Phase 2: Smart Batching)
adaptive_batch_coordinator:
  enabled: false                      # DISABLED: Enable after testing
  python_rpc_method: 'adaptive_batch.update_metrics'
  update_interval_ms: 1000            # Metric update frequency (1 second)
  default_batch_size: 4               # Fallback batch size
  min_batch_size: 2                   # Minimum batch size
  max_batch_size: 16                  # Maximum batch size
  rpc_timeout_ms: 100                 # RPC timeout (100ms)

# Retry Policy (Phase 2: Fault Tolerance)
retry_policy:
  enabled: false                      # DISABLED: Enable after testing
  max_attempts: 3                     # Maximum retry attempts
  initial_delay_ms: 100               # Initial backoff delay
  max_delay_ms: 5000                  # Maximum backoff delay
  backoff_multiplier: 2.0             # Exponential multiplier
  jitter: 0.25                        # Jitter factor (±25%)
  retryable_errors:
    - 'TIMEOUT'
    - 'ECONNRESET'
    - 'EPIPE'
    - 'WORKER_RESTART'

# Circuit Breaker (Phase 2: Fault Tolerance)
circuit_breaker:
  enabled: false                      # DISABLED: Enable after testing
  failure_threshold: 5                # Failures to trigger OPEN
  recovery_timeout_ms: 10000          # Time in OPEN before HALF_OPEN (10 seconds)
  half_open_max_calls: 1              # Max calls in HALF_OPEN
  half_open_success_threshold: 2      # Successes to return to CLOSED
  failure_window_ms: 60000            # Rolling window for failures (1 minute)

# Phase 3: Production Hardening (Performance Optimization v0.2.0)
# Connection pooling, streaming optimization, model lifecycle management, zero-downtime restarts

# Connection Pool (Phase 3.1)
connection_pool:
  enabled: false                      # DISABLED: Enable after testing
  min_connections: 2                  # Minimum warm connections per worker
  max_connections: 10                 # Maximum connections per worker
  acquire_timeout_ms: 5000            # Timeout to acquire connection (5 seconds)
  idle_timeout_ms: 60000              # Idle connection timeout (1 minute)
  health_check_interval_ms: 30000     # Health check interval (30 seconds)
  warmup_on_start: true               # Pre-create connections on startup

# Streaming Controller (Phase 3.2)
streaming_controller:
  enabled: false                      # DISABLED: Enable after testing
  chunk_size_bytes: 65536             # Aggregation boundary (64KB)
  chunk_timeout_ms: 100               # Max wait for chunk (100ms)
  max_unacked_chunks: 100             # Backpressure threshold
  ack_timeout_ms: 5000                # Timeout waiting for ACK (5 seconds)
  slow_consumer_threshold_ms: 1000    # Warn if latency > 1 second
  metrics_export_interval_ms: 10000   # Metric export frequency (10 seconds)

# Model Lifecycle Manager (Phase 3.3)
model_lifecycle_manager:
  enabled: false                      # DISABLED: Enable after testing
  idle_timeout_ms: 300000             # Unload if unused > 5 minutes
  max_loaded_models: 5                # Max models in memory (LRU eviction)
  prefetch_enabled: true              # Enable predictive prefetch
  prefetch_min_confidence: 0.7        # Min confidence to prefetch (70%)
  warmups_on_startup: []              # Models to preload on engine start
  pinned_models: []                   # Models never to unload
  drain_timeout_ms: 30000             # Max time to drain requests (30 seconds)

# Rolling Restart Coordinator (Phase 3.4)
rolling_restart_coordinator:
  enabled: false                      # DISABLED: Enable after testing
  drain_timeout_ms: 30000             # Max drain time per worker (30 seconds)
  min_active_workers: 1               # Safety minimum (circuit breaker)
  preflight_check_enabled: true       # Health check before removing old worker
  preflight_timeout_ms: 5000          # Preflight deadline (5 seconds)
  request_replay_enabled: true        # Replay on new worker if timeout
  max_replay_attempts: 1              # Replay retry limit
  watchdog_interval_ms: 5000          # Enforce min workers (5 seconds)

# Environment-specific overrides
# These can be loaded based on NODE_ENV or PYTHON_ENV

environments:
  production:
    python_runtime:
      startup_timeout_ms: 60000  # 60 seconds for large models
      max_restarts: 5
    stream_registry:
      max_active_streams: 50

  development:
    python_runtime:
      startup_timeout_ms: 15000  # 15 seconds for faster iteration

  test:
    python_runtime:
      startup_timeout_ms: 15000  # 15 seconds for model loading
      max_restarts: 1
    json_rpc:
      default_timeout_ms: 30000  # 30 seconds for model operations
    stream_registry:
      default_timeout_ms: 30000  # 30 seconds for streaming tests

# Phase 4.2: HTTP/2 Transport Multiplexing (Performance Optimization v0.2.0)
# Replaces per-request HTTP/1.1 connections with multiplexed HTTP/2 sessions
# Target: 15-20% faster TTFT, 20% lower CPU, 40% reduction in connection overhead

# HTTP/2 Session Pool
transport:
  http2:
    enabled: false                      # DISABLED: Enable after testing
    max_sessions: 16                    # Maximum HTTP/2 sessions
    max_streams_per_session: 100        # Max streams per session (HTTP/2 spec: 100-256)
    ping_interval_ms: 15000             # Ping interval (15 seconds)
    connect_timeout_ms: 200             # Connection timeout (200ms)
    tls:
      ca_file: config/certs/ca.pem      # TLS CA certificate
      cert_file: config/certs/service.pem  # TLS service certificate
      key_file: config/certs/service.key   # TLS private key
      reject_unauthorized: true         # Enforce certificate validation

  # WebSocket Gateway (Bidirectional fallback)
  websocket:
    enabled: false                      # DISABLED: Enable after testing
    max_connections: 200                # Maximum WebSocket connections
    max_frame_size_bytes: 1048576       # Max frame size (1MB)
    idle_timeout_ms: 600000             # Idle timeout (10 minutes)
    heartbeat_interval_ms: 30000        # Heartbeat interval (30 seconds)

  # SSE Writer Configuration (Zero-copy optimization)
  sse_writer:
    chunk_pool:
      max_pool_size: 1000               # Maximum chunk pool size
      chunk_size: 65536                 # Chunk size (64KB)
    backpressure_threshold: 1048576     # Backpressure threshold (1MB)
    max_buffered_chunks: 100            # Max buffered chunks before backpressure

