# kr-serve-mlx v0.2.0 - Architecture Agent Deep Analysis

> **Generated by**: AutomatosX Architecture Agent (Avery)
> **Date**: 2025-11-03
> **Duration**: 241.7s
> **Tokens**: 7,330 (prompt: 5873, completion: 1457)

---

## Request Batching Enhancement

### Current State Analysis
- Current queue only batches `tokenize`/`check_draft` and still dispatches serial calls in Python
- IPC savings top out well below the 90% target
- **Files**: `src/core/batch-queue.ts:118`, `python/runtime.py:590`

### Recommendations

1. **Transport-Level OpsMultiplexer**
   - Group requests by method+model
   - Hold for 1–4 ms unless high-priority
   - Emit `batch_*` envelopes sized from live concurrency + latency percentiles
   - **Config**: `config/runtime.yaml:4`, `src/core/batch-queue.ts:125`

2. **True Vectorized Handlers**
   - Expose `batch_encode`/`batch_decode` on tokenizer layer
   - Run draft checks concurrently with `asyncio.gather`
   - One RPC amortizes multiple model lookups
   - **Files**: `python/models/tokenizer.py:24`, `python/runtime.py:610`

3. **Queue Pressure Integration**
   - Feed queue pressure into existing RequestQueue
   - Interactive traffic flushes immediately
   - Low-priority work stretches batches
   - Fall back to single-call mode when tail latency breaches SLO
   - **Files**: `src/core/request-queue.ts:73`, `config/runtime.yaml:10`

4. **Instrumentation**
   - Add "batched vs solo" counters
   - Track per-method hit ratios
   - Calibrate toward 90% reduction goal
   - Codify policy in new ADR
   - **Files**: `src/core/batch-queue.ts:125`

---

## Model Caching Strategy

### Current State Analysis
- Cache is purely in-process maps on TypeScript side
- Loader explicitly avoids persistence
- Every process warm-starts from disk
- **Files**: `src/core/model-manager.ts:50`, `python/models/loader.py:7`

### Recommendations

1. **Content-Addressed Artifact Store**
   - Stand up under `cacheDir`
   - Normalize HuggingFace snapshots
   - Handle quantization variants
   - Manage tokenizer assets for reuse across restarts
   - **Files**: `src/core/model-manager.ts:31`

2. **Model Supervisor Daemon** ⭐ KEY INNOVATION
   - Lightweight Python daemon per host
   - Unix domain socket communication
   - Multiple Node runtimes lease handles from same MLX process
   - Avoids duplicating weights
   - Keep stdio mode as fallback for isolated tenants
   - **Config**: `config/runtime.yaml:33`

3. **Shared-Memory Weight Pool**
   - `mmap`-backed weight pool
   - Repeated loads map same pages read-only
   - Guardrails for incompatible dtypes
   - Explicit eviction hooks
   - **Files**: `python/models/loader.py:108`

4. **Cache Health Tracking**
   - Monitor hit/miss rates
   - Track warm time
   - Monitor memory residency
   - Drive eviction heuristics
   - Record design as **ADR-011 "Shared Model Artifact Cache"**

---

## Enhanced Error Handling

### Current State Analysis
- Engine errors are normalized
- No retry/circuit policy
- Repeated transport faults propagate directly to callers
- **Files**: `src/api/errors.ts:20`, `src/api/errors.ts:52`

### Recommendations

1. **Idempotency-Based Retry Policy**
   - Classify methods by idempotency
   - Wrap transport with bounded retries + jitter
   - Safe operations: tokenize, check_draft, runtime info
   - Leave `generate` single-shot (non-idempotent)
   - **Implementation**: Add policy hook in `src/bridge/jsonrpc-transport.ts`

2. **Circuit Breakers**
   - Per-method circuit breakers
   - Key off consecutive failures
   - Integrate with Python runtime auto-restart
   - Fail fast instead of queuing doomed work
   - **Config**: `config/runtime.yaml:27`, `src/core/request-queue.ts:140`

3. **Centralized Recovery Flows**
   - Retryable timeouts → requeue via BatchQueue
   - Non-retryable faults → emit structured recovery events
   - Client can observe recovery status
   - **Files**: `src/core/batch-queue.ts:197`

4. **Enhanced Error Payloads**
   - Add correlation IDs
   - Attach to telemetry for triage
   - Structured error information

---

## Telemetry & Monitoring

### Current State Analysis
- No first-class telemetry
- Config toggles for profiling disabled by default
- No counters feed out
- **Config**: `config/runtime.yaml:58`

### Recommendations

1. **OpenTelemetry Integration**
   - Embed metrics/tracing in transport, queues, batching
   - Record RPC latency
   - Track queue depth
   - Monitor batch flush stats
   - Measure stream TTFT/throughput
   - **Files**:
     - `src/core/batch-queue.ts:125`
     - `src/core/request-queue.ts:109`
     - `python/models/generator.py:200`

2. **Metrics Endpoint**
   - Expose `/metrics` endpoint (Prometheus or OTLP)
   - Node endpoint for TypeScript metrics
   - Minimal collector in Python runtime
   - Surface MLX memory, cache residency, tokens/sec
   - **Files**: `python/runtime.py:197`

3. **Structured Logging**
   - Capture logs with correlation IDs
   - Ship alongside traces
   - Reconstruct failure paths
   - Make telemetry configuration part of governance ADR

---

## Stream Optimization

### Current State Analysis
- StreamRegistry tracks handles in memory
- Fixed max streams
- Relies on EventEmitter with no spill control
- **Files**: `src/bridge/stream-registry.ts:111`, `config/runtime.yaml:38`

### Recommendations

1. **Adaptive Limits**
   - Throttle new streams when queue latency or TTFT crosses thresholds
   - Offer backpressure callbacks to client API
   - **Files**: `src/bridge/stream-registry.ts:145`

2. **Chunk Pooling**
   - Implement chunk pooling
   - Optional gzip framing
   - Shrink memory churn from cumulative strings
   - Especially important for long generations
   - **Files**: `python/models/generator.py:234`

3. **ACK/Credit Flow**
   - Explicit ACK/credit flow from TypeScript to Python
   - `queue.put` waits reflect consumer readiness
   - Replace fixed retry loops
   - **Files**: `python/models/generator.py:216`

4. **Per-Stream Metrics**
   - TTFT tracking
   - Tokens/sec monitoring
   - Cancellation tracking
   - Expiration audits
   - Spot slow consumers early
   - **Files**: `src/bridge/stream-registry.ts:186`

---

## Effort & Roadmap (Sprint-Based)

### Phase 0: Foundation (1 sprint - 2 weeks)
**Goal**: Baseline instrumentation + metrics plumbing

- Set up OpenTelemetry infrastructure
- Add initial metrics collection points
- Create Prometheus endpoint
- Establish telemetry governance ADR
- **Prerequisite**: Required for proving ROI on batching and caching

### Phase 1: Core Performance (2 sprints - 4 weeks)
**Goal**: Deliver 90% IPC reduction

- Transport multiplexer implementation
- Python vectorized handlers
- Batch processing enhancements
- **Deliverable**: Headline 90% IPC reduction
- **Mandates**: ADR-011/012 updates

### Phase 2: Model Caching (1-1.5 sprints - 2-3 weeks)
**Goal**: Cross-process model sharing

- Shared artifact cache
- Model supervisor daemon (Unix domain socket)
- mmap-backed weight pool
- Kill-switch fallback
- Soak testing under load

### Phase 3: Production Readiness (1 sprint - 2 weeks)
**Goal**: Reliability patterns

- Retry logic with jitter
- Circuit breakers
- Telemetry dashboards
- Grafana templates
- **Unlocks**: Production readiness KPIs

### Phase 4: Polish & Documentation (1 sprint - 2 weeks)
**Goal**: Complete the package

- Stream backpressure enhancements
- Documentation refresh
- Test coverage to 80%+
- Standards alignment

**Total Estimated Duration**: 6.5-7.5 sprints (~13-15 weeks)

---

## Architecture Decision Records (ADRs)

### Required New ADRs

1. **ADR-011: Shared Model Artifact Cache**
   - Content-addressed artifact store design
   - Model supervisor daemon architecture
   - mmap-backed weight sharing
   - Cache eviction policies
   - Health tracking metrics

2. **ADR-012: Request Batching Strategy**
   - Transport-level multiplexing
   - Vectorized handler protocols
   - Queue pressure integration
   - Batching policy and thresholds

3. **ADR-013: Telemetry & Observability**
   - OpenTelemetry integration patterns
   - Metrics collection points
   - Structured logging with correlation IDs
   - Telemetry governance

4. **ADR-014: Error Handling & Resilience**
   - Idempotency classification
   - Retry policies with jitter
   - Circuit breaker patterns
   - Recovery flows

---

## Key Innovations

### 1. Model Supervisor Daemon ⭐
**Innovation**: Per-host Python daemon managing shared MLX process

**Benefits**:
- Multiple Node processes share single MLX runtime
- Eliminates weight duplication
- Dramatically reduces memory footprint
- Unix domain socket for fast IPC
- Stdio fallback for isolated tenants

**Implementation Complexity**: Medium-High
**ROI**: Very High (60-90% memory reduction for multi-process deployments)

### 2. Transport-Level OpsMultiplexer
**Innovation**: Group and batch requests at transport layer

**Benefits**:
- Method+model aware batching
- Adaptive timing (1-4ms) based on priority
- Live concurrency feedback
- 90% IPC reduction achievable

**Implementation Complexity**: Medium
**ROI**: Very High (90% IPC reduction target)

### 3. Adaptive Stream Limits
**Innovation**: Dynamic stream throttling based on performance

**Benefits**:
- Prevents resource exhaustion
- Client backpressure signals
- Early slow consumer detection
- Improved stability under load

**Implementation Complexity**: Low-Medium
**ROI**: High (prevents cascading failures)

---

## Risk Mitigation

### High-Risk Areas

1. **Model Supervisor Daemon**
   - **Risk**: Single point of failure, complex failure modes
   - **Mitigation**: Stdio fallback mode, health checks, auto-restart
   - **Testing**: Chaos engineering tests (kill daemon mid-request)

2. **Vectorized Batching Correctness**
   - **Risk**: Batched results may differ from unbatched
   - **Mitigation**: Extensive correctness testing, bit-for-bit comparison
   - **Testing**: Property-based tests, regression suite

3. **Circuit Breaker Cascades**
   - **Risk**: Circuit breakers may open unnecessarily
   - **Mitigation**: Careful threshold tuning, health check integration
   - **Testing**: Fault injection, load testing

### Medium-Risk Areas

1. **OpenTelemetry Overhead**
   - **Risk**: Telemetry adds latency
   - **Mitigation**: Sampling, async export, configurable
   - **Testing**: Performance benchmarks with/without telemetry

2. **mmap Compatibility**
   - **Risk**: dtype incompatibilities, platform differences
   - **Mitigation**: Validation layer, explicit error handling
   - **Testing**: Cross-platform testing, dtype matrix

---

## Next Steps

1. **Review with CTO (Tony)** - Runway allocation for 6.5-7.5 sprints
2. **Socialize ADR Additions** - Get feedback on ADR-011 through ADR-014
3. **Seed Metrics Stories** - Create user stories for each phase
4. **Validate Milestones** - Ensure each milestone has clear success criteria
5. **Resource Planning** - Assign developers, set up code review process

---

## Conclusion

The architecture agent analysis provides **concrete, actionable recommendations** that build on existing code rather than requiring rewrites. Key innovations like the **Model Supervisor Daemon** and **Transport-Level OpsMultiplexer** are targeted solutions that address specific bottlenecks.

**Confidence Assessment**: ⭐⭐⭐⭐⭐ (5/5)

The phased approach ensures we can:
- Deliver value incrementally
- Validate assumptions early
- Adjust course based on metrics
- Maintain backward compatibility

**Great architecture is invisible—it lets teams move fast and stay aligned.**

---

**Generated by**: AutomatosX Architecture Agent (Avery)
**Reviewed by**: Claude Code
**Status**: Ready for Technical Review ✅
